{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to import census CSVs to SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputpath = 'to_import'\n",
    "outputpath = 'sql'\n",
    "flavour = 'mysql'\n",
    "insertdata = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the CSV Files and make .SQL files from them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get csv files to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files(inputpath):\n",
    "    try:\n",
    "        files = [f for f in listdir(inputpath) if isfile(join(inputpath, f))]\n",
    "        return(files)\n",
    "    except:\n",
    "        print('Couldn\\'t get files for some reason')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2016Census_G01_AUS_SA4.csv', '2016Census_G02_AUS_SA4.csv', '2016Census_G03_AUS_SA4.csv', '2016Census_G04A_AUS_SA4.csv', '2016Census_G04B_AUS_SA4.csv']\n"
     ]
    }
   ],
   "source": [
    "# Test get_files()\n",
    "files = get_files(inputpath)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to actually make the schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_table_schema(file, inputpath, outputpath, flavour):\n",
    "    try:\n",
    "        # set to mysql, postgresql, etc\n",
    "        command = 'csvsql --dialect ' + flavour + ' '\n",
    "        # use the first 10k lines to guess column type\n",
    "        command += '--snifflimit 10000' + ' '\n",
    "        # feed input and output paths\n",
    "        command += inputpath + '/' + file + ' > ' + outputpath + '/' + file[:-4] + '.sql'\n",
    "        \n",
    "        return command\n",
    "    \n",
    "    except:\n",
    "        print('Couldn\\'t make command to build table schema for file: ' + file)\n",
    "\n",
    "def make_table_schemas(files, inputpath = 'to_import', outputpath = 'sql', flavour = 'mysql'):\n",
    "    for file in files:\n",
    "        try:\n",
    "            #print(file)\n",
    "            command = make_table_schema(file, inputpath, outputpath, flavour)\n",
    "            #print(command)\n",
    "            os.system(command)\n",
    "            print('Made schema for: ' + file)\n",
    "        except:\n",
    "            print('Fell over making schema for: ' + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go! Make the schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made schema for: 2016Census_G01_AUS_SA4.csv\n",
      "Made schema for: 2016Census_G02_AUS_SA4.csv\n",
      "Made schema for: 2016Census_G03_AUS_SA4.csv\n",
      "Made schema for: 2016Census_G04A_AUS_SA4.csv\n",
      "Made schema for: 2016Census_G04B_AUS_SA4.csv\n"
     ]
    }
   ],
   "source": [
    "import csvkit\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "make_table_schemas(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Execute the .SQL Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the .SQL file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sql_files(path = 'sql'):\n",
    "    try:\n",
    "        files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "        return(files)\n",
    "    except:\n",
    "        print('Couldn\\'t get the sql files for some reason')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the .SQL file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sql_file_contents(file, filepath=False):\n",
    "    if filepath:\n",
    "        file = filepath + '/' + file\n",
    "    \n",
    "    try:\n",
    "        fd = open(file, 'r')\n",
    "        sqlFile = fd.read()\n",
    "        fd.close()\n",
    "        return sqlFile\n",
    "    except:\n",
    "        print('Couldn\\'t get the sql file contents for some reason: ' + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to monkey patch the average columns - explained in readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def monkey_patch_averages(contents):\n",
    "    contents = contents.replace('`Average_num_psns_per_bedroom` DECIMAL NOT NULL', '`Average_num_psns_per_bedroom` DECIMAL (4,2) NOT NULL')\n",
    "    contents = contents.replace('`Average_household_size` DECIMAL NOT NULL', '`Average_household_size` DECIMAL (4,2) NOT NULL')\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to execute the .SQL files contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute_sql_file_contents(connection, contents):\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(contents)\n",
    "\n",
    "        connection.commit()\n",
    "    except:\n",
    "         print('SQL Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually run it all - Execute the .SQL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For instance we found this one: 2016Census_G01_AUS_SA4.sql\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from db import host, port, user, password, db\n",
    "# these variables are stored in the file db.py so they aren't listed in GIT!\n",
    "# you could define them here if you are just using Jupyter on a laptop\n",
    "\n",
    "# make sql connection\n",
    "connection = pymysql.connect(host=host,\n",
    "                             port=port,\n",
    "                             user=user,\n",
    "                             password=password,\n",
    "                             db=db,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "# get the list of sql files we made before\n",
    "sql_files = get_sql_files()\n",
    "print('For instance we found this one: ' + sql_files[0])\n",
    "\n",
    "# loop through them\n",
    "for sql_file in sql_files:\n",
    "    try:\n",
    "        # read them\n",
    "        query = get_sql_file_contents(sql_file, filepath='sql')\n",
    "        query = monkey_patch_averages(query)\n",
    "        execute_sql_file_contents(connection, query)\n",
    "    except:\n",
    "        'Failed to execute sql for some reason' + sql_file\n",
    "\n",
    "# close connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data from the .CSVs into the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create a mysql connection string to use with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mysql_engine_string(user,password,host,db,port=3306):\n",
    "    enginestr = 'mysql://'\n",
    "    enginestr += user\n",
    "    enginestr += ':'\n",
    "    enginestr += password\n",
    "    enginestr += '@'\n",
    "    enginestr += host\n",
    "    enginestr += ':'\n",
    "    enginestr += str(port)\n",
    "    enginestr += '/'\n",
    "    enginestr += db\n",
    "    \n",
    "    return enginestr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to take a filename, read it into a Pandas dataframe, and write that dataframe to a mysql table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_into_mysql(file, engine, path=False):\n",
    "    \n",
    "    try:\n",
    "        tablename = file[:-4] # strip '.csv'\n",
    "\n",
    "        if path:\n",
    "            file = path + '/' + file\n",
    "        \n",
    "        #header=0 makes it treat the first row as headers\n",
    "        df = pandas.read_csv(file, header=0, sep=',')\n",
    "        \n",
    "        #if_exists = append means insert into\n",
    "        #index=False means don't try to write the Pandas index as a column\n",
    "        df.to_sql(con=engine, name=tablename, if_exists='append', index=False)   \n",
    "    except:\n",
    "        print('Error: Couldn\\'t insert into mysql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually run it - Insert data into the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for instance we found: 2016Census_G01_AUS_SA4.csv\n",
      "Inserted data for: 2016Census_G01_AUS_SA4.csv\n",
      "Inserted data for: 2016Census_G02_AUS_SA4.csv\n",
      "Inserted data for: 2016Census_G03_AUS_SA4.csv\n",
      "Inserted data for: 2016Census_G04A_AUS_SA4.csv\n",
      "Inserted data for: 2016Census_G04B_AUS_SA4.csv\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas\n",
    "\n",
    "# these both exist if you ran these in one hit, but here in case\n",
    "inputpath = 'to_import'\n",
    "files = get_files(inputpath)\n",
    "print('for instance we found: ' + files[0])\n",
    "\n",
    "# again, user, password et al are defined in db.py\n",
    "engine = create_mysql_engine_string(user,password,host,db,port)\n",
    "\n",
    "# for each .sql, write\n",
    "for file in files:\n",
    "    insert_into_mysql(file, engine, path=inputpath)\n",
    "    print('Inserted data for: ' + file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
